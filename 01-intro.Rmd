# Fundamentals of Bayesian Inference

The first few chapters of Gelman's text are introductory, and we attempt to highlight the key definitions and summarize each chapter. At the end of each chapter we attempt several problems.  Probability and inference is defined using three steps

1. setting up the full probability model for a joint distribution for all observable and unobservable quantities.
2. Conditioning on observed data: computing the appropriate *posterior* distribution, the conditional probability distribution of the unobserved quantities of oltimate interest, given the observed data.
3. Evaluating the fit of the model.

## General notation for statistical inference

There are two different kinds of estimands, the first are potentially observable quantities, such as future observations of a process, and the second are quantities that are not directly observable, namely the parameters that govern a process being investigated.  

### Exchangeability {-}
One key assumption is that the n values $y_i$ are regarded as *exchangeable*, meaning that the uncertainty can be expressed as a joint probability $p(y_1,...,y_n)$ that is invariant to permutations of indexes.  Often times the exchangeable distribution is modeled as *iid*.

### Explanatory variables {-}
It is common to have observations on each unit which have non-random variables called *explanatory variables* or *covariates*.  The explanatory variables are usually denoted by X.  However treating X as random then exchangeability can be extended $(x,y)_i$ which is invariant to permutations of the indexes.  Further, it is always appropriate to assume exchangeability of y, conditioned on sufficient information of X, where the indexes can be thought of as randomly assigned.  It follows that if two units have the same value of x, then the distributions of y are the same.

### Hierarchical modeling {-}
for a model across patients across different cities, we can assume exchangability to patients within a city.  Further conditioned on the explanatory variables at the individual, the conditional distribution given these explanatory variables would be exchangeable.


## Bayesian inference
The prior, p($\theta$), and the sampling distribution, or the *data distribution*, $p(y|\theta)$ is

$$ p(\theta,y) = p(\theta)p(y | \theta)$$

Where using Bayes' rule the posterior distribution 
\begin{equation}
p(\theta | y ) = \frac{p(\theta)p(y| \theta)}{p(y)}
(\#eq:bayes)
\end{equation}

Where $p(y)  = \int p(\theta)p(y | \theta)d\theta$, or a sum in discrete case.  An equivalent form of \@ref(eq:bayes) is the *unnormalized posterior density* given as

\begin{equation}
p(\theta | y ) \propto p(\theta)p(y | \theta)
(\#eq:unnorm)
\end{equation}

Note that $p(y | \theta)$ is taken as a function of $\theta$, not of y.

## Prediction {-}
Inferences about an unknown *observable* variable, are called predictive inferences.  Beofre the data y are considered, the distribution of the unknown, observable, y is $$ p(y) = \int p(y,\theta)d\theta = \int p(\theta)p(y | \theta)d\theta$$

this is defined as the marginal distribution of y, and also called *prior predictive distribution*.  Prior refers that the data is not conditional on any previous observation, and predictive refers to the data being observable.

The *posterior predictive distribution* is conditional on the observed y, but is predictive because it is predicting observable values.

\begin{equation}
\begin{split}
p(\hat{y} | y) &= \int p(\hat{y}, \theta | y) d\theta \\
&= \int p(\hat{y} |\theta, y) p(\theta|y) d\theta \\
& = \int p(\hat{y}| \theta) p(\theta | y) d\theta \\
\end{split}
(\#eq:postpred)
\end{equation}

## Likelihood {-}
The data *y* affects the posterior inference only through \@ref(eq:unnorm) likelihood function $p(y| \theta)$ which is regarded as a function of $\theta$ for fixed y.  The *likelihood function* is defined as $p(y | \theta)$, and the *likelihood principle* is for any given sample, and any two likelihood models $p(y | \theta)$, two models with the same likelihood will have the same inference for $\theta$.

## Subjectivity and Objectivity {-}

The frequentist models, MLEs, have subjectivity in their assumptions because they rely on long sequence of identical trials, that are iid.  The Bayesian model relies on the prior distribution.  If any experiment is repeatable and can replicated, the the prior distribution can be estimated from the data themselves and the analysis is more 'objective'.  Replication increases objectivity of a given model.  However the Bayesian approach allows for (1) the ability to combine information from multiple sources (allowing for greater objectivity) and (2) more encompassing by accounting for uncertainity about the unknowns in a statistical problem.

It is important to include as much background information as possible 

## Exercises

1.  (a) for $\sigma=2$; $p(y) = \int p(y|\theta)p(\theta)d\theta = (1/2)N(1,\sigma^2)+ (1/2)N(2,\sigma^2)$ as the marginal density
```{r}
 fy<-function(y) {return(0.5*dnorm(y,mean=1,sd=2)+0.5*dnorm(y,2,sd=2))}    
```

  (b) $P(\theta=1 | y=1) = \frac{p(\theta=1)p(y|\theta=1)}{p(y)}= \frac{(1/2)N(1,4)}{(1/2)N(1,\sigma^2)+ (1/2)N(2,\sigma^2)}$ = 0.53 
    
```{r}
dy<-function(y){ return( (1/2)*dnorm(y,mean=1,sd=2)/fy(y))}
dy(1)
```


