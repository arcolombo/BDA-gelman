[["cross.html", "Chapter 3 Single parameter models 3.1 Estimating a probability from binomial data 3.2 Posterior as a compromise between data and prior information 3.3 Summarizing the posterior inference 3.4 Informative prior distributions", " Chapter 3 Single parameter models 3.1 Estimating a probability from binomial data \\[\\begin{equation} p(y | \\theta) = {n \\choose y}\\theta^y(1-\\theta)^{n-y} \\tag{3.1} \\end{equation}\\] To perform Bayesian inference we assume \\(\\theta \\sim U(0,1)\\) where the posterior is \\[\\begin{equation} p(\\theta | y) \\propto \\theta^y(1-\\theta)^{n-y} \\tag{3.2} \\end{equation}\\] which is the form of a beta distribution \\(\\theta | y \\sim Beta(y+1, n-y+1)\\) 3.2 Posterior as a compromise between data and prior information The posterior is less variable than the prior because it incorporates the information from the data. \\[\\begin{equation} E(\\theta) = E(E(\\theta | y)) \\tag{3.3} \\end{equation}\\] \\[\\begin{equation} V(\\theta) = E(V(\\theta|y)) + V(E(\\theta | y)) \\tag{3.4} \\end{equation}\\] where \\(\\theta|y\\) is the posterior. So the average of the prior, is the average of the posterior means over the distribution of possible data. The variance of the prior (3.4) says the posterior variance is on average smaller than the prior variance. 3.3 Summarizing the posterior inference The mean, median, mode, and standard deviation of the posterior distribution summarize the all the current information about a model. Posterior quantiles and intervals The posterior uncertainty can be reported by presenting the quantiles of the posterior distribution. The interval, a central interval of posterior probability corresponds to the case of 100(\\(1-\\alpha)\\%\\), to the range of values above and below which lies exactly 100(\\(\\alpha/2)\\%\\) of the posterior probability. The interval estimates are posterior intervals. This differences from the confidence interval because the confidence interval is not a probability interval, because either the parameter is within the region or it is not, but the confidence interval provides information in the long run over repeated experimentation as to how many experiments would contain the true parameter. There is also the highest posterior interval which is a probabilistic interval that is not less than any region outside of the interval. 3.4 Informative prior distributions the property that the posterior distribution follows the same parametric form as the prior distribution is called conjugacy. Where the beta prior distribution is a conjugate family for the binomial likelihood. so given the binomial likelihood \\(p(y|\\theta)\\propto \\theta^a(1-\\theta)^b\\), and a prior density \\(p(\\theta)\\propto \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\) the posterior is of the beta family. \\[ \\begin{aligned} p(\\theta | y) &amp;\\propto \\theta^y(1-\\theta)^{n-y}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\\\ &amp;= \\theta^{y+\\alpha-1}(1-\\theta)^{n-y+\\beta-1}\\\\ &amp;= Beta(\\theta | \\alpha+y, \\beta+n-y) \\end{aligned} \\] Conjugate prior distributions Conjugacy is formally defined as if F is a class of sampling distributions \\(p(y|\\theta)\\), and P is a class of prior distributions for \\(\\theta\\), then the class P is conjugate for F if \\(p(\\theta | y) \\in P\\) for all \\(p(.|\\theta)\\in F\\) and \\(p(.)\\in P\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
