<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Asymptotics and connections to non-Bayesian approaches | Bayesian Data Analysis</title>
  <meta name="description" content="Chapter 4 Asymptotics and connections to non-Bayesian approaches | Bayesian Data Analysis review by Andrew Gelman, supervised by Dr. Paul Marjoram" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Asymptotics and connections to non-Bayesian approaches | Bayesian Data Analysis" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Chapter 4 Asymptotics and connections to non-Bayesian approaches | Bayesian Data Analysis review by Andrew Gelman, supervised by Dr. Paul Marjoram" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Asymptotics and connections to non-Bayesian approaches | Bayesian Data Analysis" />
  
  <meta name="twitter:description" content="Chapter 4 Asymptotics and connections to non-Bayesian approaches | Bayesian Data Analysis review by Andrew Gelman, supervised by Dr. Paul Marjoram" />
  

<meta name="author" content="Anthony R. Colombo, Dr. Paul Marjoram" />


<meta name="date" content="2022-08-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="exercises-2.html"/>
<link rel="next" href="blocks.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#independent-study"><i class="fa fa-check"></i>Independent study</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#supervised-learning"><i class="fa fa-check"></i>Supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="fundamentals-of-bayesian-inference.html"><a href="fundamentals-of-bayesian-inference.html"><i class="fa fa-check"></i><b>1</b> Fundamentals of Bayesian Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="fundamentals-of-bayesian-inference.html"><a href="fundamentals-of-bayesian-inference.html#general-notation-for-statistical-inference"><i class="fa fa-check"></i><b>1.1</b> General notation for statistical inference</a>
<ul>
<li class="chapter" data-level="" data-path="fundamentals-of-bayesian-inference.html"><a href="fundamentals-of-bayesian-inference.html#exchangeability"><i class="fa fa-check"></i>Exchangeability</a></li>
<li class="chapter" data-level="" data-path="fundamentals-of-bayesian-inference.html"><a href="fundamentals-of-bayesian-inference.html#explanatory-variables"><i class="fa fa-check"></i>Explanatory variables</a></li>
<li class="chapter" data-level="" data-path="fundamentals-of-bayesian-inference.html"><a href="fundamentals-of-bayesian-inference.html#hierarchical-modeling"><i class="fa fa-check"></i>Hierarchical modeling</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="fundamentals-of-bayesian-inference.html"><a href="fundamentals-of-bayesian-inference.html#bayesian-inference"><i class="fa fa-check"></i><b>1.2</b> Bayesian inference</a></li>
<li class="chapter" data-level="" data-path="fundamentals-of-bayesian-inference.html"><a href="fundamentals-of-bayesian-inference.html#prediction"><i class="fa fa-check"></i>Prediction</a></li>
<li class="chapter" data-level="" data-path="fundamentals-of-bayesian-inference.html"><a href="fundamentals-of-bayesian-inference.html#likelihood"><i class="fa fa-check"></i>Likelihood</a></li>
<li class="chapter" data-level="" data-path="fundamentals-of-bayesian-inference.html"><a href="fundamentals-of-bayesian-inference.html#subjectivity-and-objectivity"><i class="fa fa-check"></i>Subjectivity and Objectivity</a></li>
<li class="chapter" data-level="1.3" data-path="fundamentals-of-bayesian-inference.html"><a href="fundamentals-of-bayesian-inference.html#exercises"><i class="fa fa-check"></i><b>1.3</b> Exercises</a></li>
<li class="chapter" data-level="" data-path="fundamentals-of-bayesian-inference.html"><a href="fundamentals-of-bayesian-inference.html#simulation-100-times"><i class="fa fa-check"></i>Simulation 100 times</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="cross.html"><a href="cross.html"><i class="fa fa-check"></i><b>2</b> Single parameter models</a>
<ul>
<li class="chapter" data-level="2.1" data-path="cross.html"><a href="cross.html#estimating-a-probability-from-binomial-data"><i class="fa fa-check"></i><b>2.1</b> Estimating a probability from binomial data</a></li>
<li class="chapter" data-level="2.2" data-path="cross.html"><a href="cross.html#posterior-as-a-compromise-between-data-and-prior-information"><i class="fa fa-check"></i><b>2.2</b> Posterior as a compromise between data and prior information</a></li>
<li class="chapter" data-level="2.3" data-path="cross.html"><a href="cross.html#summarizing-the-posterior-inference"><i class="fa fa-check"></i><b>2.3</b> Summarizing the posterior inference</a>
<ul>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#posterior-quantiles-and-intervals"><i class="fa fa-check"></i>Posterior quantiles and intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="cross.html"><a href="cross.html#informative-prior-distributions"><i class="fa fa-check"></i><b>2.4</b> Informative prior distributions</a>
<ul>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#conjugate-prior-distributions"><i class="fa fa-check"></i>Conjugate prior distributions</a></li>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#conjugate-prior-distributions-exponential-families-and-sufficient-statistics"><i class="fa fa-check"></i>Conjugate prior, distributions, exponential families, and sufficient statistics</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="cross.html"><a href="cross.html#normal-distribution-with-known-variance"><i class="fa fa-check"></i><b>2.5</b> Normal distribution with known variance</a>
<ul>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#likelihood-of-one-data-point"><i class="fa fa-check"></i>Likelihood of one data point</a></li>
<li class="chapter" data-level="2.5.1" data-path="cross.html"><a href="cross.html#conjugate-prior-and-posterior-distributions"><i class="fa fa-check"></i><b>2.5.1</b> Conjugate prior and posterior distributions</a></li>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#posterior-predictive-distribution"><i class="fa fa-check"></i>Posterior predictive distribution</a></li>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#normal-model-with-multiple-observations"><i class="fa fa-check"></i>Normal model with multiple observations</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="cross.html"><a href="cross.html#other-standard-single-parameter-models"><i class="fa fa-check"></i><b>2.6</b> Other standard single-parameter models</a>
<ul>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#normal-distribution-with-known-mean-but-unknown-variance"><i class="fa fa-check"></i>Normal distribution with known mean but unknown variance</a></li>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#poisson-model"><i class="fa fa-check"></i>Poisson model</a></li>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#negative-binomial-distribution"><i class="fa fa-check"></i>Negative Binomial distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="cross.html"><a href="cross.html#exercises-1"><i class="fa fa-check"></i><b>2.7</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#question-1"><i class="fa fa-check"></i>Question 1</a></li>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#normal-approximation-example"><i class="fa fa-check"></i>Normal approximation example</a></li>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#question-3"><i class="fa fa-check"></i>Question 3</a></li>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#question-4"><i class="fa fa-check"></i>Question 4</a></li>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#question-7"><i class="fa fa-check"></i>Question 7</a></li>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#question-8-normal-distribution-with-unknown-mean"><i class="fa fa-check"></i>Question 8 (Normal distribution with unknown mean)</a></li>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#question-10"><i class="fa fa-check"></i>Question 10</a></li>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#question-11"><i class="fa fa-check"></i>Question 11</a></li>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#question-12"><i class="fa fa-check"></i>Question 12</a></li>
<li class="chapter" data-level="" data-path="cross.html"><a href="cross.html#question-13"><i class="fa fa-check"></i>Question 13</a></li>
<li class="chapter" data-level="2.7.1" data-path="cross.html"><a href="cross.html#question-21"><i class="fa fa-check"></i><b>2.7.1</b> Question 21</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter-3.html"><a href="chapter-3.html"><i class="fa fa-check"></i><b>3</b> Chapter 3</a></li>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i>Exercises</a>
<ul>
<li class="chapter" data-level="3.1" data-path="exercises-2.html"><a href="exercises-2.html#question-1-1"><i class="fa fa-check"></i><b>3.1</b> Question 1</a></li>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#question-2"><i class="fa fa-check"></i>Question 2</a></li>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#question-3-1"><i class="fa fa-check"></i>Question 3</a></li>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#question-4-independent-binomial"><i class="fa fa-check"></i>Question 4 (independent binomial)</a>
<ul>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#question-4-dirichlet"><i class="fa fa-check"></i>Question 4 (dirichlet)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#question-5"><i class="fa fa-check"></i>Question 5</a></li>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#question-7-1"><i class="fa fa-check"></i>Question 7</a></li>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#question-8"><i class="fa fa-check"></i>question 8</a></li>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#question-9"><i class="fa fa-check"></i>question 9</a></li>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#question-11-1"><i class="fa fa-check"></i>Question 11</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="asymptotics-and-connections-to-non-bayesian-approaches.html"><a href="asymptotics-and-connections-to-non-bayesian-approaches.html"><i class="fa fa-check"></i><b>4</b> Asymptotics and connections to non-Bayesian approaches</a>
<ul>
<li class="chapter" data-level="4.1" data-path="asymptotics-and-connections-to-non-bayesian-approaches.html"><a href="asymptotics-and-connections-to-non-bayesian-approaches.html#normal-approximations-to-the-posterior-distribution"><i class="fa fa-check"></i><b>4.1</b> Normal approximations to the posterior distribution</a>
<ul>
<li class="chapter" data-level="" data-path="asymptotics-and-connections-to-non-bayesian-approaches.html"><a href="asymptotics-and-connections-to-non-bayesian-approaches.html#summarizing-posterior-distributions-by-point-estimates-and-standard-errors"><i class="fa fa-check"></i>Summarizing posterior distributions by point estimates and standard errors</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="asymptotics-and-connections-to-non-bayesian-approaches.html"><a href="asymptotics-and-connections-to-non-bayesian-approaches.html#large-sample-theory"><i class="fa fa-check"></i><b>4.2</b> Large-sample theory</a>
<ul>
<li class="chapter" data-level="" data-path="asymptotics-and-connections-to-non-bayesian-approaches.html"><a href="asymptotics-and-connections-to-non-bayesian-approaches.html#asymptotic-normality-and-consistency"><i class="fa fa-check"></i>Asymptotic normality and consistency</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="asymptotics-and-connections-to-non-bayesian-approaches.html"><a href="asymptotics-and-connections-to-non-bayesian-approaches.html#frequency-evaluations-of-bayesian-inferences"><i class="fa fa-check"></i><b>4.3</b> Frequency evaluations of Bayesian inferences</a>
<ul>
<li class="chapter" data-level="" data-path="asymptotics-and-connections-to-non-bayesian-approaches.html"><a href="asymptotics-and-connections-to-non-bayesian-approaches.html#large-sample-correspondence"><i class="fa fa-check"></i>Large sample correspondence</a></li>
<li class="chapter" data-level="" data-path="asymptotics-and-connections-to-non-bayesian-approaches.html"><a href="asymptotics-and-connections-to-non-bayesian-approaches.html#point-estimation-consistency-and-efficiency"><i class="fa fa-check"></i>Point estimation, consistency, and efficiency</a></li>
<li class="chapter" data-level="" data-path="asymptotics-and-connections-to-non-bayesian-approaches.html"><a href="asymptotics-and-connections-to-non-bayesian-approaches.html#confidence-coverage"><i class="fa fa-check"></i>Confidence coverage</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="asymptotics-and-connections-to-non-bayesian-approaches.html"><a href="asymptotics-and-connections-to-non-bayesian-approaches.html#exercises-3"><i class="fa fa-check"></i><b>4.4</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="asymptotics-and-connections-to-non-bayesian-approaches.html"><a href="asymptotics-and-connections-to-non-bayesian-approaches.html#question-1-2"><i class="fa fa-check"></i>Question 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="blocks.html"><a href="blocks.html"><i class="fa fa-check"></i><b>5</b> Blocks</a>
<ul>
<li class="chapter" data-level="5.1" data-path="blocks.html"><a href="blocks.html#equations"><i class="fa fa-check"></i><b>5.1</b> Equations</a></li>
<li class="chapter" data-level="5.2" data-path="blocks.html"><a href="blocks.html#theorems-and-proofs"><i class="fa fa-check"></i><b>5.2</b> Theorems and proofs</a></li>
<li class="chapter" data-level="5.3" data-path="blocks.html"><a href="blocks.html#callout-blocks"><i class="fa fa-check"></i><b>5.3</b> Callout blocks</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sharing-your-book.html"><a href="sharing-your-book.html"><i class="fa fa-check"></i><b>6</b> Sharing your book</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sharing-your-book.html"><a href="sharing-your-book.html#publishing"><i class="fa fa-check"></i><b>6.1</b> Publishing</a></li>
<li class="chapter" data-level="6.2" data-path="sharing-your-book.html"><a href="sharing-your-book.html#pages"><i class="fa fa-check"></i><b>6.2</b> 404 pages</a></li>
<li class="chapter" data-level="6.3" data-path="sharing-your-book.html"><a href="sharing-your-book.html#metadata-for-sharing"><i class="fa fa-check"></i><b>6.3</b> Metadata for sharing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="7" data-path="parts.html"><a href="parts.html"><i class="fa fa-check"></i><b>7</b> Parts</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="asymptotics-and-connections-to-non-bayesian-approaches" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Asymptotics and connections to non-Bayesian approaches<a href="asymptotics-and-connections-to-non-bayesian-approaches.html#asymptotics-and-connections-to-non-bayesian-approaches" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="normal-approximations-to-the-posterior-distribution" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Normal approximations to the posterior distribution<a href="asymptotics-and-connections-to-non-bayesian-approaches.html#normal-approximations-to-the-posterior-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If the posterior distribution is unimodal and roughly symmetric, it can be approximated by a normal distribution, such that the logarithm of the posterior density is approximated by a quadratic function via the Taylor series expansion of <span class="math inline">\(\theta\)</span>.</p>
<p>Consider a quadratic approximation to the log-posterior centered on the posterior mode. where the linear term goes to 0.</p>
<p><span class="math display" id="eq:logdensityquadratic">\[\begin{equation}
log p(\theta|y) = log p(\hat{\theta|y}) + (1/2)(\theta-\hat{\theta)})^T[ d^2/d\theta^2 log p(\theta|y)]_{\theta=\hat{\theta}}+...
\tag{4.1}
\end{equation}\]</span></p>
<p>The first term is a constant and the second term is proportional to the normal density yielding the approximation and we can expand the posterior density second derivative in terms of the prior and likelihood.</p>
<p><span class="math display">\[\begin{equation}
[d^2/d\theta^2 log p(\theta|y)]_{\theta=\hat{\theta}} =[d^2/d\theta^2log p(\theta)]_{\theta=\hat{\theta}}+\sum_{i=1}^n[d^2/d\theta^2log p(y_i|\theta)]_{\theta=\hat{\theta}}
\end{equation}\]</span></p>
<p>Now to show the normal approximation</p>
<p><span class="math display">\[
\begin{aligned}
log p(\theta|y) &amp;= log p(\hat{\theta|y}) + (1/2)(\theta-\hat{\theta)})^T[ d^2/d\theta^2 log p(\theta|y)]_{\theta=\hat{\theta}}+... \\
&amp;= log p(\hat{\theta|y}) + (1/2)(\theta-\hat{\theta)})^T ( d^2/d\theta^2 log p(\hat{\theta}) + \sum_{i=1}^n d^2/d\theta^2 log p(y_i | \theta)_{\theta=\hat{\theta}} ) \\
log p(\theta|y)  - log p(\hat{\theta|y}) &amp;= + (1/2)(\theta-\hat{\theta)})^T (c  -n* J(\theta_0)) \\
  p(\theta|y ) - p(\hat{\theta}|y) &amp;\propto exp( -\frac{1}{2(nJ(\theta_0))^{-1}}(\theta-\hat{\theta})^T)
\end{aligned}
\]</span>
Which taking the limit as <span class="math inline">\(|\theta -\hat{\theta}| \to 0\)</span>, then the posterior converges to 0 written as <span class="math inline">\(p(\theta|y )- p(\hat{\theta}|y) \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span>, and we have normality.</p>
<p>In discussing large-sample periorties, the concept of Fisher Information , <span class="math inline">\(J(\theta)\)</span>, in the context of Jeffreys’ prior is used.</p>
<p><span class="math display" id="eq:normalapprox">\[\begin{equation}
p(\theta|y) \approx N(\hat{\theta}, [I(\hat{\theta})]^{-1})
\tag{4.2}
\end{equation}\]</span></p>
<p>Where I(<span class="math inline">\(\theta)\)</span> is the <em>observed information</em>
<span class="math display">\[\begin{equation}
I(\theta)= -d^2/d\theta^2 log p(\theta|y)
\end{equation}\]</span></p>
<p>Where is the mode <span class="math inline">\(\hat{\theta}\)</span> is in the interior of the parameter space, then the information is positive definite.</p>
<div id="summarizing-posterior-distributions-by-point-estimates-and-standard-errors" class="section level3 unnumbered hasAnchor">
<h3>Summarizing posterior distributions by point estimates and standard errors<a href="asymptotics-and-connections-to-non-bayesian-approaches.html#summarizing-posterior-distributions-by-point-estimates-and-standard-errors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>From the asymptotic theory, if n is large enough, a posterior distribution is approximated by the normal distribution. A standard inferential summary is the 95<span class="math inline">\(\%\)</span> interval obtained by computing a point estimate <span class="math inline">\(\hat{\theta}\)</span> such as the MLE (which is the posterior under a uniform prior density), plus or minus two standard errors, with the standard error estimated from the information at the estimate <span class="math inline">\(I(\hat{\theta})\)</span>.</p>
</div>
</div>
<div id="large-sample-theory" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Large-sample theory<a href="asymptotics-and-connections-to-non-bayesian-approaches.html#large-sample-theory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The basic tool of Bayesian inference is asymptotic normality of the posterior distribution, as more data arrive from the same underlying process, the posterior distribution of the parameter vector approaches multivariate normality. Suppose the data are modeled by a parametric family, <span class="math inline">\(p(y|\theta)\)</span>, and a prior p<span class="math inline">\((\theta)\)</span>, and suppose that the true distribution is included in the parametric family (i.e. if <span class="math inline">\(f(y) = p(y|\theta_0))\)</span> then the property of asymptotic normality and <em>consistency</em> holds.</p>
<p>Consistency is defined as the posterior distribution converges to a point mass at the true parameter, <span class="math inline">\(\theta_0\)</span> as n<span class="math inline">\(\to \infty\)</span>.</p>
<p>Note, that if the true distribution is not included in the parametric family, then there is no longer a true parameter to converge to. One must use the <em>Kullback-Leibler divergence</em> to determine the value <span class="math inline">\(\theta_0\)</span> that makes the model distribution closest to the true distribution.</p>
<div id="asymptotic-normality-and-consistency" class="section level3 unnumbered hasAnchor">
<h3>Asymptotic normality and consistency<a href="asymptotics-and-connections-to-non-bayesian-approaches.html#asymptotic-normality-and-consistency" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Under regularity conditions ( the likelihood is a continuous function of <span class="math inline">\(\theta\)</span>, and that <span class="math inline">\(\theta_0\)</span> is not a boundary point), as <span class="math inline">\(n\to \infty\)</span>, the posterior distribution of <span class="math inline">\(\theta\)</span> approaches normality with mean <span class="math inline">\(\theta_0\)</span> and variance (<span class="math inline">\(nJ(\theta_0))^{-1}. Where J(\)</span>)$ is the <em>FIsher information</em> in context fo Jeffreys’ Prior.</p>
<p>The posterior mode is consistent for <span class="math inline">\(\theta_0\)</span>, as <span class="math inline">\(n\to \infty\)</span>, so the mass of the posterior <span class="math inline">\(p(\theta|y)\)</span> becomes concentrated in small neighborhoods of <span class="math inline">\(\theta_0\)</span> and the distance of <span class="math inline">\(|\theta-\theta_0|\to 0\)</span>.</p>
<p>Further, we can write the coefficient of the quadratic term in <a href="asymptotics-and-connections-to-non-bayesian-approaches.html#eq:logdensityquadratic">(4.1)</a>.
<span class="math display">\[\begin{equation}
[d^2/d\theta^2 log p(\theta|y)]_{\theta=\hat{\theta}} =[d^2/d\theta^2log p(\theta)]_{\theta=\hat{\theta}}+\sum_{i=1}^n[d^2/d\theta^2log p(y_i|\theta)]_{\theta=\hat{\theta}}
\end{equation}\]</span></p>
<p>This is considered a function of <span class="math inline">\(\theta\)</span>, as a constant term plus the sum of n terms whose expected value under the true sampling distribution <span class="math inline">\(p(y_i|\theta_0\)</span>)., is approximately <span class="math inline">\(-J(\theta_0)\)</span>, assuming <span class="math inline">\(\hat{\theta}\)</span> is close to <span class="math inline">\(\theta\)</span>.</p>
<p>In summary, as the limit of n, in the context of a family of models posterior mode, <span class="math inline">\(\hat{\theta}\)</span>, approaches the truth <span class="math inline">\(\theta_0\)</span>, and the curvature approaches <span class="math inline">\(nJ(\hat{\theta})\)</span> or nJ(<span class="math inline">\(\theta_0\)</span>). Interesting, as <span class="math inline">\(n\to\infty\)</span> the prior term is a constant, and the likelihood dominates the posterior because the likelihood alone is used to obtain the mode and curvature for the normal approximation.</p>
</div>
</div>
<div id="frequency-evaluations-of-bayesian-inferences" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Frequency evaluations of Bayesian inferences<a href="asymptotics-and-connections-to-non-bayesian-approaches.html#frequency-evaluations-of-bayesian-inferences" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The notion of <em>stable estimation</em> which says that for a fixed model, the posterior approaches a point as more data arrive, leading , in the limit, to inferential certainty, is based on the concepts of repeated sampling. It is certainly appealing that if the hypothesized family of probability models contain the true distribution, then as more information about <span class="math inline">\(\theta\)</span> arrives, the posterior distribution converges to the true value of <span class="math inline">\(\theta\)</span>.</p>
<div id="large-sample-correspondence" class="section level3 unnumbered hasAnchor">
<h3>Large sample correspondence<a href="asymptotics-and-connections-to-non-bayesian-approaches.html#large-sample-correspondence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose that the normal approximation holds <a href="asymptotics-and-connections-to-non-bayesian-approaches.html#eq:normalapprox">(4.2)</a> for the posterior distribution for <span class="math inline">\(\theta\)</span>, then we can transform to the <em>standard</em> normal multivariate normal</p>
<p><span class="math display" id="eq:standardmultivariateapprox">\[\begin{equation}
[I(\hat{\theta})]^{1/2}(\theta-\hat{\theta}) |y \sim N(0,I)
\tag{4.3}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(\hat{\theta}\)</span> is the posterior mode and <span class="math inline">\([I(\hat{\theta})]^{1/2}\)</span> is any matrix square root of the observed fisher information. In addition to <span class="math inline">\(\hat{\theta}\to \theta_0\)</span> we can write the approximation using I(<span class="math inline">\(\theta_0\)</span>). If the true data distribution is included in the class of models, so that <span class="math inline">\(f(y)= p(y|\theta)\)</span>, then under <em>repeated sampling</em> with fixed <span class="math inline">\(\theta\)</span>, as <span class="math inline">\(n \to \infty\)</span> then</p>
<p><span class="math display" id="eq:classical">\[\begin{equation}
[I(\hat{\theta})]^{1/2}(\theta-\hat{\theta}) |\theta \sim N(0,I)
\tag{4.4}
\end{equation}\]</span></p>
<p>This is generally proven for the MLE, but can be extended for the posterior mode <span class="math inline">\(\hat{\theta}\)</span>. This results suggest that for any function (<span class="math inline">\(\theta-\hat{\theta})\)</span> the posterior distribution derived from <a href="asymptotics-and-connections-to-non-bayesian-approaches.html#eq:standardmultivariateapprox">(4.3)</a> is asymptotically the same as the repeated sampling distribution from <a href="asymptotics-and-connections-to-non-bayesian-approaches.html#eq:classical">(4.4)</a>. Thus for a <span class="math inline">\(95\%\)</span> central posterior interval for <span class="math inline">\(\theta\)</span> will cover the true value 95<span class="math inline">\(\%\)</span> of the time under repeated sampling with any fixed true <span class="math inline">\(\theta.\)</span></p>
</div>
<div id="point-estimation-consistency-and-efficiency" class="section level3 unnumbered hasAnchor">
<h3>Point estimation, consistency, and efficiency<a href="asymptotics-and-connections-to-non-bayesian-approaches.html#point-estimation-consistency-and-efficiency" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For large samples, obtaining an estimate- makes most sense when the posterior mode <span class="math inline">\(\hat{\theta}\)</span> is the obvious center and the <span class="math inline">\(nI(\theta_0\)</span>) is small and practically unimportant. However in smaller samples, one can define optimal point estimates, but it is better to show the full representation of the full posterior distribution. In most problems, the point estimate and the standard error are adequate to summarize the posterior inference. We interpret the estimate as an inferential summary, not as a decision solution / classification.</p>
<p>A point estimate is said to be <em>consistent</em> as the samples get larger, it converges to the true value parameter. Thus if <span class="math inline">\(f(y)=p(y|\theta_0)\)</span>, then a point estimate <span class="math inline">\(\hat{\theta}\)</span> of <span class="math inline">\(\theta\)</span> is consistent if its sampling distribution converges to a point mass at <span class="math inline">\(\theta_0\)</span> for <span class="math inline">\(n\to \infty\)</span>.</p>
<p><em>Asymptotic unbiasedness</em> is defined as <span class="math inline">\((E(\hat{\theta}|\theta_0 - \theta_0))/sd(\hat{\theta}|\theta_0)\)</span> converges to 0 as sample size increases.</p>
<p><em>Efficiency</em> for a point estimate is if there is no other function of y that estimates <span class="math inline">\(\theta\)</span> with lower mean squared error, that is if the expression E((<span class="math inline">\(\hat{\theta}-\theta_0)^2|\theta_0)\)</span> is at its optiomal lowest value. An estimate is asymptotically efficient if its efficiency approaches 1 as the sample size n, increases to infinity.</p>
</div>
<div id="confidence-coverage" class="section level3 unnumbered hasAnchor">
<h3>Confidence coverage<a href="asymptotics-and-connections-to-non-bayesian-approaches.html#confidence-coverage" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If a region C(y) includes <span class="math inline">\(\theta_0\)</span> at least 100(<span class="math inline">\(1-\alpha)\%\)</span> of the time, then C(y) is called the 100(<span class="math inline">\(1-\alpha)\%\)</span> <em>confidence region</em> for parameter <span class="math inline">\(\theta\)</span>. We saw previously that asymptotically a 100(<span class="math inline">\(1-\alpha)\%\)</span> central posterior interval for <span class="math inline">\(\theta\)</span> has the property that , in repeated samples of y, 100(<span class="math inline">\(1-\alpha)\%\)</span> of the intervals include <span class="math inline">\(\theta_0\)</span>.</p>
</div>
</div>
<div id="exercises-3" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Exercises<a href="asymptotics-and-connections-to-non-bayesian-approaches.html#exercises-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="question-1-2" class="section level3 unnumbered hasAnchor">
<h3>Question 1<a href="asymptotics-and-connections-to-non-bayesian-approaches.html#question-1-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>a using simple calculus we found l’ = <span class="math inline">\(\frac{\sum_{i=1}^5(y_i-\theta)}{1+(y_i-\theta)^2}\)</span> and l’’ = <span class="math inline">\(\frac{-2}{(1+(y_i-\theta)^2)^2}\)</span></li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="exercises-2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="blocks.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/04-Asymptotics-and-connections-to-nonBayesian-approaches.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
